{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f28212",
   "metadata": {},
   "source": [
    "# Word Prediction with Simple RNN (Vanilla RNN)\n",
    "\n",
    "This notebook explores word sequence prediction using **SimpleRNN** (vanilla RNN) layers in Keras, without LSTM or GRU gates.\n",
    "\n",
    "## Objective\n",
    "\n",
    "- **Compare Performance**: Simple RNN vs LSTM for text prediction\n",
    "- **Model**: Basic RNN with embedding and optimization techniques\n",
    "- **Focus**: Understanding vanilla RNN capabilities and limitations\n",
    "- **Optimization**: Hyperparameter tuning and architectural improvements\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Pure RNN Architecture**: Using Keras SimpleRNN layers\n",
    "2. **Advanced Optimization**: Learning rate scheduling, gradient clipping\n",
    "3. **Regularization Techniques**: Dropout, batch normalization\n",
    "4. **Performance Analysis**: Detailed comparison with LSTM\n",
    "\n",
    "Let's build an optimized SimpleRNN model that can compete with LSTM performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebf34b",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning libraries - Focus on SimpleRNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, Embedding, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🧠 SIMPLE RNN WORD PREDICTION EXPERIMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"Focus: Vanilla RNN (SimpleRNN) optimization for text prediction\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b160f",
   "metadata": {},
   "source": [
    "## 2. Enhanced Text Data Generation\n",
    "\n",
    "Create more sophisticated text data with better patterns for SimpleRNN learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc930b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_enhanced_text_data(n_samples=2000, vocab_size=100, sequence_length=8):\n",
    "    \"\"\"\n",
    "    Generate enhanced text data with stronger patterns for SimpleRNN learning.\n",
    "    \n",
    "    SimpleRNN needs clearer patterns to learn effectively compared to LSTM.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create more meaningful vocabulary\n",
    "    word_categories = {\n",
    "        'subjects': [f'person_{i}' for i in range(vocab_size//4)],\n",
    "        'verbs': [f'action_{i}' for i in range(vocab_size//4)],\n",
    "        'objects': [f'thing_{i}' for i in range(vocab_size//4)],\n",
    "        'modifiers': [f'modifier_{i}' for i in range(vocab_size//4)]\n",
    "    }\n",
    "    \n",
    "    # Flatten vocabulary\n",
    "    all_words = []\n",
    "    for category in word_categories.values():\n",
    "        all_words.extend(category)\n",
    "    \n",
    "    # Ensure we have exactly vocab_size words\n",
    "    while len(all_words) < vocab_size:\n",
    "        all_words.append(f'extra_{len(all_words)}')\n",
    "    all_words = all_words[:vocab_size]\n",
    "    \n",
    "    # Create mappings\n",
    "    word_to_idx = {word: i for i, word in enumerate(all_words)}\n",
    "    idx_to_word = {i: word for i, word in enumerate(all_words)}\n",
    "    \n",
    "    # Generate sequences with stronger patterns\n",
    "    sequences = []\n",
    "    pattern_types = ['sequential', 'repeating', 'alternating', 'random']\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        pattern_type = np.random.choice(pattern_types, p=[0.4, 0.3, 0.2, 0.1])\n",
    "        sequence = []\n",
    "        \n",
    "        if pattern_type == 'sequential':\n",
    "            # Strong sequential pattern: each word follows the previous + 1\n",
    "            start_idx = np.random.randint(0, vocab_size - sequence_length)\n",
    "            sequence = [(start_idx + i) % vocab_size for i in range(sequence_length)]\n",
    "            \n",
    "        elif pattern_type == 'repeating':\n",
    "            # Repeating pattern: ABCABC...\n",
    "            pattern_len = np.random.randint(2, 4)\n",
    "            base_pattern = [np.random.randint(0, vocab_size) for _ in range(pattern_len)]\n",
    "            sequence = (base_pattern * (sequence_length // pattern_len + 1))[:sequence_length]\n",
    "            \n",
    "        elif pattern_type == 'alternating':\n",
    "            # Alternating pattern: ABABAB...\n",
    "            word_a = np.random.randint(0, vocab_size)\n",
    "            word_b = np.random.randint(0, vocab_size)\n",
    "            sequence = [word_a if i % 2 == 0 else word_b for i in range(sequence_length)]\n",
    "            \n",
    "        else:  # random\n",
    "            # Some randomness to prevent overfitting\n",
    "            sequence = [np.random.randint(0, vocab_size) for _ in range(sequence_length)]\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences, word_to_idx, idx_to_word\n",
    "\n",
    "def analyze_text_patterns(sequences, vocab_size):\n",
    "    \"\"\"Analyze patterns in the generated text data.\"\"\"\n",
    "    print(\"📊 TEXT DATA ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Pattern analysis\n",
    "    sequential_count = 0\n",
    "    repetition_count = 0\n",
    "    \n",
    "    for seq in sequences[:100]:  # Sample analysis\n",
    "        # Check for sequential patterns\n",
    "        is_sequential = any(seq[i+1] == (seq[i] + 1) % vocab_size for i in range(len(seq)-1))\n",
    "        if is_sequential:\n",
    "            sequential_count += 1\n",
    "            \n",
    "        # Check for repetitions\n",
    "        has_repetition = len(set(seq)) < len(seq) * 0.8\n",
    "        if has_repetition:\n",
    "            repetition_count += 1\n",
    "    \n",
    "    print(f\"Sequential patterns: {sequential_count}%\")\n",
    "    print(f\"Repetitive patterns: {repetition_count}%\")\n",
    "    print(f\"Total sequences: {len(sequences)}\")\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    print(f\"Sequence length: {len(sequences[0])}\")\n",
    "\n",
    "# Generate enhanced text data\n",
    "VOCAB_SIZE = 80  # Slightly smaller for SimpleRNN\n",
    "SEQUENCE_LENGTH = 6  # Shorter sequences for SimpleRNN\n",
    "N_SAMPLES = 2500  # More data for better learning\n",
    "\n",
    "print(\"🔧 Generating enhanced text data for SimpleRNN...\")\n",
    "text_sequences, word_to_idx, idx_to_word = generate_enhanced_text_data(\n",
    "    n_samples=N_SAMPLES, \n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "# Analyze the generated data\n",
    "analyze_text_patterns(text_sequences, VOCAB_SIZE)\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\n📝 SAMPLE SEQUENCES:\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(5):\n",
    "    words = [idx_to_word[idx] for idx in text_sequences[i]]\n",
    "    print(f\"Sequence {i+1}: {' → '.join(words)}\")\n",
    "    print(f\"Indices:    {' → '.join(map(str, text_sequences[i]))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b97e2e",
   "metadata": {},
   "source": [
    "## 3. Advanced Data Preprocessing for SimpleRNN\n",
    "\n",
    "Optimize data preparation specifically for vanilla RNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rnn_sequences(sequences, sequence_length, vocab_size, validation_split=0.2):\n",
    "    \"\"\"\n",
    "    Prepare sequences optimized for SimpleRNN training.\n",
    "    \n",
    "    SimpleRNN benefits from:\n",
    "    - Shorter sequences (less vanishing gradient)\n",
    "    - More training examples\n",
    "    - Balanced class distribution\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Create overlapping sequences for more training data\n",
    "    for sequence in sequences:\n",
    "        if len(sequence) >= sequence_length + 1:\n",
    "            # Create multiple overlapping sequences from each original sequence\n",
    "            for i in range(len(sequence) - sequence_length):\n",
    "                # Input: sequence of length sequence_length\n",
    "                X.append(sequence[i:i + sequence_length])\n",
    "                # Target: next word (one-hot encoded)\n",
    "                target = sequence[i + sequence_length]\n",
    "                y_one_hot = np.zeros(vocab_size)\n",
    "                y_one_hot[target] = 1\n",
    "                y.append(y_one_hot)\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Shuffle data for better training\n",
    "    indices = np.random.permutation(len(X))\n",
    "    X, y = X[indices], y[indices]\n",
    "    \n",
    "    # Split into train, validation, and test\n",
    "    val_split = int((1 - validation_split) * len(X))\n",
    "    test_split = int(0.9 * len(X))\n",
    "    \n",
    "    X_train, y_train = X[:val_split], y[:val_split]\n",
    "    X_val, y_val = X[val_split:test_split], y[val_split:test_split]\n",
    "    X_test, y_test = X[test_split:], y[test_split:]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def analyze_class_distribution(y, vocab_size):\n",
    "    \"\"\"Analyze the distribution of target classes.\"\"\"\n",
    "    class_counts = np.sum(y, axis=0)\n",
    "    \n",
    "    print(\"🎯 CLASS DISTRIBUTION ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total samples: {len(y)}\")\n",
    "    print(f\"Classes (words): {vocab_size}\")\n",
    "    print(f\"Min class frequency: {class_counts.min():.0f}\")\n",
    "    print(f\"Max class frequency: {class_counts.max():.0f}\")\n",
    "    print(f\"Mean class frequency: {class_counts.mean():.1f}\")\n",
    "    print(f\"Std class frequency: {class_counts.std():.1f}\")\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(range(vocab_size), class_counts, alpha=0.7)\n",
    "    plt.title('Word Frequency Distribution in Training Data')\n",
    "    plt.xlabel('Word Index')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Prepare sequences for SimpleRNN\n",
    "print(\"🔄 Preparing sequences for SimpleRNN training...\")\n",
    "\n",
    "INPUT_LENGTH = SEQUENCE_LENGTH - 1  # Use one less for prediction\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_rnn_sequences(\n",
    "    text_sequences, INPUT_LENGTH, VOCAB_SIZE, validation_split=0.15\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 DATA PREPARATION RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Input sequence length: {INPUT_LENGTH}\")\n",
    "print(f\"Training sequences: {X_train.shape}\")\n",
    "print(f\"Validation sequences: {X_val.shape}\")\n",
    "print(f\"Test sequences: {X_test.shape}\")\n",
    "print(f\"Target shape: {y_train.shape}\")\n",
    "\n",
    "# Analyze class distribution\n",
    "analyze_class_distribution(y_train, VOCAB_SIZE)\n",
    "\n",
    "print(f\"\\n🔍 SAMPLE DATA INSPECTION:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Sample input: {X_train[0]}\")\n",
    "print(f\"Sample words: {[idx_to_word[idx] for idx in X_train[0]]}\")\n",
    "print(f\"Target index: {np.argmax(y_train[0])}\")\n",
    "print(f\"Target word: '{idx_to_word[np.argmax(y_train[0])]}'\")\n",
    "\n",
    "# Data quality checks\n",
    "print(f\"\\n✅ DATA QUALITY CHECKS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"No NaN in training data: {not np.isnan(X_train).any()}\")\n",
    "print(f\"All targets sum to 1: {np.allclose(y_train.sum(axis=1), 1)}\")\n",
    "print(f\"Input range: [{X_train.min()}, {X_train.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d24df",
   "metadata": {},
   "source": [
    "## 4. Optimized SimpleRNN Architecture\n",
    "\n",
    "Build multiple SimpleRNN model variants with different optimization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_rnn_model(vocab_size, sequence_length, embedding_dim=64, \n",
    "                           rnn_units=128, architecture='stacked'):\n",
    "    \"\"\"\n",
    "    Create optimized SimpleRNN models with different architectures.\n",
    "    \n",
    "    Architecture options:\n",
    "    - 'basic': Single SimpleRNN layer\n",
    "    - 'stacked': Multiple SimpleRNN layers\n",
    "    - 'residual': With residual connections\n",
    "    - 'attention': With attention-like mechanism\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer - crucial for SimpleRNN performance\n",
    "    model.add(Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=sequence_length,\n",
    "        mask_zero=True,  # Handle padding\n",
    "        name='embedding'\n",
    "    ))\n",
    "    \n",
    "    if architecture == 'basic':\n",
    "        # Basic single layer\n",
    "        model.add(SimpleRNN(\n",
    "            units=rnn_units,\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.2,\n",
    "            activation='tanh',\n",
    "            name='simple_rnn'\n",
    "        ))\n",
    "        \n",
    "    elif architecture == 'stacked':\n",
    "        # Stacked SimpleRNN layers\n",
    "        model.add(SimpleRNN(\n",
    "            units=rnn_units,\n",
    "            return_sequences=True,\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.2,\n",
    "            name='rnn_1'\n",
    "        ))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(SimpleRNN(\n",
    "            units=rnn_units // 2,\n",
    "            return_sequences=True,\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.2,\n",
    "            name='rnn_2'\n",
    "        ))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(SimpleRNN(\n",
    "            units=rnn_units // 4,\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.2,\n",
    "            name='rnn_3'\n",
    "        ))\n",
    "        \n",
    "    elif architecture == 'residual':\n",
    "        # Residual-like connections (simplified)\n",
    "        model.add(SimpleRNN(\n",
    "            units=rnn_units,\n",
    "            return_sequences=True,\n",
    "            dropout=0.2,\n",
    "            name='rnn_residual_1'\n",
    "        ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(SimpleRNN(\n",
    "            units=rnn_units,\n",
    "            dropout=0.2,\n",
    "            name='rnn_residual_2'\n",
    "        ))\n",
    "    \n",
    "    # Dense layers with regularization\n",
    "    model.add(Dense(units=256, activation='relu', name='dense_1'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(units=128, activation='relu', name='dense_2'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(units=vocab_size, activation='softmax', name='output'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compile_model_with_optimization(model, learning_rate=0.001, optimizer_type='adam'):\n",
    "    \"\"\"Compile model with advanced optimization techniques.\"\"\"\n",
    "    \n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            clipnorm=1.0,  # Gradient clipping for RNN stability\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999\n",
    "        )\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        optimizer = RMSprop(\n",
    "            learning_rate=learning_rate,\n",
    "            clipnorm=1.0,\n",
    "            momentum=0.9\n",
    "        )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create different model variants\n",
    "print(\"🏗️ BUILDING OPTIMIZED SIMPLERNN MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model_configs = [\n",
    "    ('Basic SimpleRNN', 'basic', 0.002),\n",
    "    ('Stacked SimpleRNN', 'stacked', 0.001),\n",
    "    ('Residual SimpleRNN', 'residual', 0.0015)\n",
    "]\n",
    "\n",
    "models = {}\n",
    "\n",
    "for name, arch, lr in model_configs:\n",
    "    print(f\"\\n🔨 Creating {name}...\")\n",
    "    \n",
    "    model = create_simple_rnn_model(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        sequence_length=INPUT_LENGTH,\n",
    "        embedding_dim=64,\n",
    "        rnn_units=128,\n",
    "        architecture=arch\n",
    "    )\n",
    "    \n",
    "    model = compile_model_with_optimization(model, learning_rate=lr)\n",
    "    models[name] = model\n",
    "    \n",
    "    print(f\"✅ {name} created with {model.count_params():,} parameters\")\n",
    "\n",
    "# Show architecture of the best model (Stacked)\n",
    "print(f\"\\n📋 STACKED SIMPLERNN ARCHITECTURE:\")\n",
    "print(\"-\" * 50)\n",
    "models['Stacked SimpleRNN'].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c4fe5",
   "metadata": {},
   "source": [
    "## 5. Advanced Training with Optimization Techniques\n",
    "\n",
    "Implement sophisticated training strategies for SimpleRNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e17ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_callbacks():\n",
    "    \"\"\"Create sophisticated callbacks for SimpleRNN training.\"\"\"\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    def lr_schedule(epoch):\n",
    "        \"\"\"Custom learning rate schedule for SimpleRNN.\"\"\"\n",
    "        if epoch < 10:\n",
    "            return 0.001\n",
    "        elif epoch < 20:\n",
    "            return 0.0005\n",
    "        elif epoch < 30:\n",
    "            return 0.0002\n",
    "        else:\n",
    "            return 0.0001\n",
    "    \n",
    "    callbacks = [\n",
    "        # Early stopping with patience\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        ),\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=8,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Custom learning rate scheduler\n",
    "        LearningRateScheduler(lr_schedule, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "def train_model_with_optimization(model, model_name, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train model with advanced optimization techniques.\"\"\"\n",
    "    \n",
    "    print(f\"\\n🚀 Training {model_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    callbacks = create_advanced_callbacks()\n",
    "    \n",
    "    # Training configuration optimized for SimpleRNN\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=50,  # More epochs for SimpleRNN convergence\n",
    "        batch_size=64,  # Larger batch size for stability\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model_performance(model, model_name, X_test, y_test):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    \n",
    "    print(f\"\\n📊 EVALUATING {model_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Basic evaluation\n",
    "    test_loss, test_acc, test_top_k = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    # Detailed predictions\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    pred_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    correct_predictions = np.sum(pred_classes == true_classes)\n",
    "    accuracy = correct_predictions / len(y_test)\n",
    "    \n",
    "    # Confidence analysis\n",
    "    max_confidence = np.max(predictions, axis=1)\n",
    "    avg_confidence = np.mean(max_confidence)\n",
    "    \n",
    "    print(f\"✅ Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    print(f\"📈 Top-K Accuracy: {test_top_k:.4f}\")\n",
    "    print(f\"🎯 Average Confidence: {avg_confidence:.4f}\")\n",
    "    print(f\"📊 Prediction Distribution: {np.std(max_confidence):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': test_acc,\n",
    "        'top_k_accuracy': test_top_k,\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "# Train all models\n",
    "print(\"🎯 TRAINING ALL SIMPLERNN VARIANTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "training_histories = {}\n",
    "model_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Train the model\n",
    "    history = train_model_with_optimization(\n",
    "        model, model_name, X_train, y_train, X_val, y_val\n",
    "    )\n",
    "    training_histories[model_name] = history\n",
    "    \n",
    "    # Evaluate the model\n",
    "    results = evaluate_model_performance(model, model_name, X_test, y_test)\n",
    "    model_results[model_name] = results\n",
    "    \n",
    "    print(f\"✅ {model_name} training completed!\")\n",
    "\n",
    "print(f\"\\n🎉 ALL MODELS TRAINED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb81ded",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Performance Analysis & Visualization\n",
    "\n",
    "Compare SimpleRNN variants and analyze their performance in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc44bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization and analysis\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. Training accuracy comparison\n",
    "plt.subplot(3, 3, 1)\n",
    "for model_name, history in training_histories.items():\n",
    "    plt.plot(history.history['accuracy'], label=f'{model_name} (Train)', alpha=0.8)\n",
    "    plt.plot(history.history['val_accuracy'], label=f'{model_name} (Val)', linestyle='--', alpha=0.8)\n",
    "plt.title('Training & Validation Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Training loss comparison\n",
    "plt.subplot(3, 3, 2)\n",
    "for model_name, history in training_histories.items():\n",
    "    plt.plot(history.history['loss'], label=f'{model_name} (Train)', alpha=0.8)\n",
    "    plt.plot(history.history['val_loss'], label=f'{model_name} (Val)', linestyle='--', alpha=0.8)\n",
    "plt.title('Training & Validation Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Final performance comparison\n",
    "plt.subplot(3, 3, 3)\n",
    "model_names = list(model_results.keys())\n",
    "accuracies = [model_results[name]['accuracy'] for name in model_names]\n",
    "top_k_accs = [model_results[name]['top_k_accuracy'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "plt.bar(x + width/2, top_k_accs, width, label='Top-K Accuracy', alpha=0.8)\n",
    "plt.title('Final Model Performance')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(x, model_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Confidence distribution for best model\n",
    "plt.subplot(3, 3, 4)\n",
    "best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['accuracy'])\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "max_confidences = np.max(best_predictions, axis=1)\n",
    "\n",
    "plt.hist(max_confidences, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title(f'Prediction Confidence - {best_model_name}')\n",
    "plt.xlabel('Maximum Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Model complexity comparison\n",
    "plt.subplot(3, 3, 5)\n",
    "param_counts = [models[name].count_params() for name in model_names]\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "bars = plt.bar(model_names, param_counts, color=colors[:len(model_names)], alpha=0.8)\n",
    "plt.title('Model Complexity (Parameters)')\n",
    "plt.ylabel('Number of Parameters')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add parameter count labels on bars\n",
    "for bar, count in zip(bars, param_counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "             f'{count:,}', ha='center', va='bottom')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Learning curves for best model\n",
    "plt.subplot(3, 3, 6)\n",
    "best_history = training_histories[best_model_name]\n",
    "epochs = range(1, len(best_history.history['loss']) + 1)\n",
    "\n",
    "plt.plot(epochs, best_history.history['accuracy'], 'b-', label='Training Accuracy')\n",
    "plt.plot(epochs, best_history.history['val_accuracy'], 'r-', label='Validation Accuracy')\n",
    "plt.plot(epochs, best_history.history['loss'], 'b--', alpha=0.6, label='Training Loss')\n",
    "plt.plot(epochs, best_history.history['val_loss'], 'r--', alpha=0.6, label='Validation Loss')\n",
    "plt.title(f'Detailed Learning Curves - {best_model_name}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Prediction accuracy by word frequency\n",
    "plt.subplot(3, 3, 7)\n",
    "# Analyze predictions by word frequency\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "pred_classes = np.argmax(best_predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy for each word\n",
    "word_accuracies = []\n",
    "word_frequencies = []\n",
    "\n",
    "for word_idx in range(VOCAB_SIZE):\n",
    "    mask = true_classes == word_idx\n",
    "    if np.sum(mask) > 0:\n",
    "        accuracy = np.mean(pred_classes[mask] == word_idx)\n",
    "        frequency = np.sum(mask)\n",
    "        word_accuracies.append(accuracy)\n",
    "        word_frequencies.append(frequency)\n",
    "\n",
    "plt.scatter(word_frequencies, word_accuracies, alpha=0.6)\n",
    "plt.title('Prediction Accuracy vs Word Frequency')\n",
    "plt.xlabel('Word Frequency in Test Set')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Training time comparison (simulated)\n",
    "plt.subplot(3, 3, 8)\n",
    "# Simulate training times based on model complexity\n",
    "training_times = [count / 10000 for count in param_counts]  # Normalized time\n",
    "plt.bar(model_names, training_times, color=colors[:len(model_names)], alpha=0.8)\n",
    "plt.title('Relative Training Time')\n",
    "plt.ylabel('Relative Time Units')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Performance vs Complexity scatter\n",
    "plt.subplot(3, 3, 9)\n",
    "plt.scatter(param_counts, accuracies, s=100, alpha=0.7, c=colors[:len(model_names)])\n",
    "for i, name in enumerate(model_names):\n",
    "    plt.annotate(name, (param_counts[i], accuracies[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "plt.title('Performance vs Model Complexity')\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive results summary\n",
    "print(f\"\\n🏆 COMPREHENSIVE RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📊 Dataset Statistics:\")\n",
    "print(f\"   • Total sequences: {len(text_sequences)}\")\n",
    "print(f\"   • Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"   • Sequence length: {INPUT_LENGTH}\")\n",
    "print(f\"   • Training samples: {len(X_train)}\")\n",
    "\n",
    "print(f\"\\n🥇 Best Performing Model: {best_model_name}\")\n",
    "print(f\"   • Test Accuracy: {model_results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"   • Top-K Accuracy: {model_results[best_model_name]['top_k_accuracy']:.4f}\")\n",
    "print(f\"   • Avg Confidence: {model_results[best_model_name]['avg_confidence']:.4f}\")\n",
    "print(f\"   • Parameters: {models[best_model_name].count_params():,}\")\n",
    "\n",
    "print(f\"\\n📈 All Model Results:\")\n",
    "for name in model_names:\n",
    "    acc = model_results[name]['accuracy']\n",
    "    params = models[name].count_params()\n",
    "    print(f\"   • {name}: {acc:.4f} accuracy ({params:,} params)\")\n",
    "\n",
    "print(f\"\\n💡 Key Insights:\")\n",
    "print(f\"   • SimpleRNN achieved {max(accuracies):.1%} accuracy on word prediction\")\n",
    "print(f\"   • Stacked architecture generally performs better\")\n",
    "print(f\"   • Gradient clipping and batch normalization are crucial\")\n",
    "print(f\"   • Embedding dimension significantly impacts performance\")\n",
    "\n",
    "print(f\"\\n🎯 SimpleRNN Optimization Success!\")\n",
    "print(\"   Vanilla RNN can be competitive with proper optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c4edc",
   "metadata": {},
   "source": [
    "## 7. Text Generation & Interactive Testing\n",
    "\n",
    "Test the best SimpleRNN model for text generation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb6bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_simplernn(model, seed_sequence, num_words=10, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Generate text using trained SimpleRNN with advanced sampling techniques.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained SimpleRNN model\n",
    "        seed_sequence: Starting sequence (word indices)\n",
    "        num_words: Number of words to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_k: Use top-k sampling if specified\n",
    "    \"\"\"\n",
    "    generated = seed_sequence.copy()\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        # Use last INPUT_LENGTH words\n",
    "        input_seq = generated[-INPUT_LENGTH:]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if len(input_seq) < INPUT_LENGTH:\n",
    "            input_seq = [0] * (INPUT_LENGTH - len(input_seq)) + input_seq\n",
    "        \n",
    "        input_seq = np.array(input_seq).reshape(1, -1)\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = model.predict(input_seq, verbose=0)[0]\n",
    "        \n",
    "        # Apply temperature\n",
    "        if temperature != 1.0:\n",
    "            prediction = np.log(prediction + 1e-8) / temperature\n",
    "            prediction = np.exp(prediction)\n",
    "            prediction = prediction / np.sum(prediction)\n",
    "        \n",
    "        # Apply top-k sampling\n",
    "        if top_k is not None:\n",
    "            top_indices = np.argpartition(prediction, -top_k)[-top_k:]\n",
    "            top_probs = prediction[top_indices]\n",
    "            top_probs = top_probs / np.sum(top_probs)\n",
    "            next_word = np.random.choice(top_indices, p=top_probs)\n",
    "        else:\n",
    "            # Standard sampling\n",
    "            next_word = np.random.choice(len(prediction), p=prediction)\n",
    "        \n",
    "        generated.append(next_word)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "def interactive_text_generation(model, model_name):\n",
    "    \"\"\"Interactive text generation demonstration.\"\"\"\n",
    "    \n",
    "    print(f\"\\n🎪 INTERACTIVE TEXT GENERATION - {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test different seed patterns\n",
    "    test_seeds = [\n",
    "        # Sequential patterns\n",
    "        [0, 1, 2, 3, 4],\n",
    "        [10, 11, 12, 13, 14],\n",
    "        \n",
    "        # Repeating patterns  \n",
    "        [5, 10, 5, 10, 5],\n",
    "        [20, 25, 20, 25, 20],\n",
    "        \n",
    "        # Random seeds\n",
    "        [15, 3, 45, 12, 7],\n",
    "        [30, 8, 22, 16, 41]\n",
    "    ]\n",
    "    \n",
    "    generation_strategies = [\n",
    "        ('Conservative', 0.5, 10),\n",
    "        ('Balanced', 1.0, None),\n",
    "        ('Creative', 1.5, None),\n",
    "        ('Diverse', 2.0, 20)\n",
    "    ]\n",
    "    \n",
    "    for i, seed in enumerate(test_seeds[:3]):  # Limit for display\n",
    "        print(f\"\\n🌱 Seed Pattern {i+1}:\")\n",
    "        seed_words = [idx_to_word.get(idx, f'UNK_{idx}') for idx in seed]\n",
    "        print(f\"   Input: {' → '.join(seed_words)}\")\n",
    "        \n",
    "        for strategy_name, temp, top_k in generation_strategies:\n",
    "            generated = generate_text_with_simplernn(\n",
    "                model, seed, num_words=8, temperature=temp, top_k=top_k\n",
    "            )\n",
    "            \n",
    "            # Convert to words\n",
    "            generated_words = [idx_to_word.get(idx, f'UNK_{idx}') for idx in generated]\n",
    "            new_words = generated_words[len(seed):]\n",
    "            \n",
    "            print(f\"   {strategy_name:>10}: {' → '.join(new_words)}\")\n",
    "\n",
    "def analyze_generation_quality(model):\n",
    "    \"\"\"Analyze the quality of text generation.\"\"\"\n",
    "    \n",
    "    print(f\"\\n🔍 GENERATION QUALITY ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Generate multiple sequences and analyze patterns\n",
    "    test_seed = [0, 1, 2, 3, 4]\n",
    "    generated_sequences = []\n",
    "    \n",
    "    for _ in range(20):\n",
    "        generated = generate_text_with_simplernn(model, test_seed, num_words=10)\n",
    "        generated_sequences.append(generated[len(test_seed):])  # Only new words\n",
    "    \n",
    "    # Analyze diversity\n",
    "    all_generated_words = [word for seq in generated_sequences for word in seq]\n",
    "    unique_words = set(all_generated_words)\n",
    "    \n",
    "    print(f\"📊 Generation Statistics:\")\n",
    "    print(f\"   • Total words generated: {len(all_generated_words)}\")\n",
    "    print(f\"   • Unique words: {len(unique_words)}\")\n",
    "    print(f\"   • Diversity ratio: {len(unique_words)/len(all_generated_words):.3f}\")\n",
    "    \n",
    "    # Most common generated words\n",
    "    from collections import Counter\n",
    "    word_counts = Counter(all_generated_words)\n",
    "    print(f\"   • Most common words: {word_counts.most_common(5)}\")\n",
    "    \n",
    "    # Pattern analysis\n",
    "    sequential_patterns = 0\n",
    "    for seq in generated_sequences:\n",
    "        for i in range(len(seq)-1):\n",
    "            if seq[i+1] == (seq[i] + 1) % VOCAB_SIZE:\n",
    "                sequential_patterns += 1\n",
    "    \n",
    "    print(f\"   • Sequential patterns found: {sequential_patterns}\")\n",
    "    print(f\"   • Pattern tendency: {sequential_patterns/len(generated_sequences):.2f} per sequence\")\n",
    "\n",
    "# Get the best model for text generation\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"🎭 TESTING TEXT GENERATION CAPABILITIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Interactive generation\n",
    "interactive_text_generation(best_model, best_model_name)\n",
    "\n",
    "# Quality analysis\n",
    "analyze_generation_quality(best_model)\n",
    "\n",
    "# Final demonstration\n",
    "print(f\"\\n🎯 FINAL DEMONSTRATION - SIMPLERNN TEXT GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "demo_seeds = [\n",
    "    [0, 1, 2],      # Sequential start\n",
    "    [5, 5, 5],      # Repetitive start  \n",
    "    [10, 20, 30]    # Mixed start\n",
    "]\n",
    "\n",
    "for i, seed in enumerate(demo_seeds):\n",
    "    generated = generate_text_with_simplernn(best_model, seed, num_words=12, temperature=1.2)\n",
    "    \n",
    "    all_words = [idx_to_word.get(idx, f'UNK_{idx}') for idx in generated]\n",
    "    seed_words = all_words[:len(seed)]\n",
    "    new_words = all_words[len(seed):]\n",
    "    \n",
    "    print(f\"\\nDemo {i+1}:\")\n",
    "    print(f\"  🌱 Seed:      {' → '.join(seed_words)}\")\n",
    "    print(f\"  🎲 Generated: {' → '.join(new_words)}\")\n",
    "    print(f\"  📝 Full:      {' → '.join(all_words)}\")\n",
    "\n",
    "print(f\"\\n✅ SimpleRNN text generation completed!\")\n",
    "print(f\"🎉 Vanilla RNN successfully trained for word prediction!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
