{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10905eef",
   "metadata": {},
   "source": [
    "# Stock Price Prediction with GRU\n",
    "\n",
    "This notebook focuses specifically on stock price time series forecasting using **GRU (Gated Recurrent Unit)** networks.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Task**: Stock price prediction using historical OHLCV data\n",
    "- **Model**: GRU-based RNN with technical indicators\n",
    "- **Data**: Synthetic stock data with realistic patterns\n",
    "- **Goal**: Build an effective stock price forecasting model\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Realistic Synthetic Data**: OHLCV data with trends, volatility, and patterns\n",
    "2. **Technical Indicators**: Moving averages, volatility measures\n",
    "3. **GRU Architecture**: Optimized for financial time series\n",
    "4. **Comprehensive Evaluation**: Multiple metrics and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d55fd",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e650cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Deep Learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Focus: Stock Price Prediction with GRU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f26866",
   "metadata": {},
   "source": [
    "## 2. Generate Stock Data with Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stock_data(n_days=1000, start_price=100.0, volatility=0.02):\n",
    "    \"\"\"Generate synthetic stock price data with realistic patterns.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate dates\n",
    "    start_date = datetime(2020, 1, 1)\n",
    "    dates = [start_date + timedelta(days=i) for i in range(n_days)]\n",
    "    \n",
    "    # Generate price data using geometric Brownian motion\n",
    "    returns = np.random.normal(0.0005, volatility, n_days)  # Small positive drift\n",
    "    prices = [start_price]\n",
    "    \n",
    "    for i in range(1, n_days):\n",
    "        price = prices[-1] * (1 + returns[i])\n",
    "        prices.append(max(price, 1.0))  # Prevent negative prices\n",
    "    \n",
    "    # Generate OHLC data\n",
    "    data = []\n",
    "    for i, (date, close) in enumerate(zip(dates, prices)):\n",
    "        # Open price (close to previous close)\n",
    "        if i == 0:\n",
    "            open_price = close\n",
    "        else:\n",
    "            open_price = prices[i-1] * (1 + np.random.normal(0, volatility/4))\n",
    "        \n",
    "        # High and low prices\n",
    "        daily_range = abs(np.random.normal(0, volatility/2))\n",
    "        high = max(open_price, close) * (1 + daily_range)\n",
    "        low = min(open_price, close) * (1 - daily_range)\n",
    "        \n",
    "        # Volume\n",
    "        volume = int(np.random.normal(1000000, 200000))\n",
    "        volume = max(volume, 100000)\n",
    "        \n",
    "        data.append({\n",
    "            'Date': date,\n",
    "            'Open': round(open_price, 2),\n",
    "            'High': round(high, 2),\n",
    "            'Low': round(low, 2),\n",
    "            'Close': round(close, 2),\n",
    "            'Volume': volume\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def add_technical_indicators(df):\n",
    "    \"\"\"Add technical indicators to stock data.\"\"\"\n",
    "    # Moving averages\n",
    "    df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['MA_10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['MA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    \n",
    "    # Price changes\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['Price_Change'] = df['Close'].diff()\n",
    "    \n",
    "    # Volatility measures\n",
    "    df['Volatility_5'] = df['Returns'].rolling(window=5).std()\n",
    "    df['Volatility_10'] = df['Returns'].rolling(window=10).std()\n",
    "    \n",
    "    # High-Low measures\n",
    "    df['HL_Range'] = (df['High'] - df['Low']) / df['Close']\n",
    "    df['OC_Range'] = (df['Close'] - df['Open']) / df['Open']\n",
    "    \n",
    "    # Volume indicators\n",
    "    df['Volume_MA'] = df['Volume'].rolling(window=10).mean()\n",
    "    df['Volume_Ratio'] = df['Volume'] / df['Volume_MA']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate and enhance stock data\n",
    "print(\"Generating stock data...\")\n",
    "stock_df = generate_stock_data(n_days=1200)\n",
    "stock_df = add_technical_indicators(stock_df)\n",
    "\n",
    "print(f\"Generated {len(stock_df)} days of stock data\")\n",
    "print(f\"Date range: {stock_df['Date'].min()} to {stock_df['Date'].max()}\")\n",
    "print(f\"Price range: ${stock_df['Close'].min():.2f} - ${stock_df['Close'].max():.2f}\")\n",
    "print(f\"Features: {list(stock_df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3433c2d6",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Sequence Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e854f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_stock_sequences(data, sequence_length=60, target_col='Close'):\n",
    "    \"\"\"Prepare stock data for RNN training.\"\"\"\n",
    "    # Select numerical features only and remove NaN values\n",
    "    feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'MA_5', 'MA_10', 'MA_20', \n",
    "                   'Volatility_5', 'Volatility_10', 'HL_Range', 'OC_Range', 'Volume_Ratio']\n",
    "    \n",
    "    # Remove columns with too many NaN values or use available features\n",
    "    available_cols = [col for col in feature_cols if col in data.columns]\n",
    "    df = data[available_cols].dropna()\n",
    "    \n",
    "    print(f\"Using features: {available_cols}\")\n",
    "    print(f\"Data shape after cleaning: {df.shape}\")\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    \n",
    "    # Find target column index\n",
    "    target_idx = available_cols.index(target_col)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(sequence_length, len(scaled_data)):\n",
    "        X.append(scaled_data[i-sequence_length:i])\n",
    "        y.append(scaled_data[i, target_idx])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Split into train, validation, and test\n",
    "    train_size = int(0.7 * len(X))\n",
    "    val_size = int(0.85 * len(X))\n",
    "    \n",
    "    X_train = X[:train_size]\n",
    "    y_train = y[:train_size]\n",
    "    X_val = X[train_size:val_size]\n",
    "    y_val = y[train_size:val_size]\n",
    "    X_test = X[val_size:]\n",
    "    y_test = y[val_size:]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler, available_cols\n",
    "\n",
    "# Prepare stock data\n",
    "SEQUENCE_LENGTH = 60\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler, features = prepare_stock_sequences(\n",
    "    stock_df, sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "print(\"Stock data preparation complete!\")\n",
    "print(f\"Training sequences: {X_train.shape}\")\n",
    "print(f\"Validation sequences: {X_val.shape}\")\n",
    "print(f\"Test sequences: {X_test.shape}\")\n",
    "print(f\"Features used: {len(features)}\")\n",
    "\n",
    "# Verify data integrity\n",
    "print(f\"\\nData integrity check:\")\n",
    "print(f\"No NaN in training data: {not np.isnan(X_train).any()}\")\n",
    "print(f\"Data range: [{X_train.min():.3f}, {X_train.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e65969",
   "metadata": {},
   "source": [
    "## 4. Build Optimized GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cfdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_gru_model(input_shape, units=[64, 32, 16], dropout_rate=0.3):\n",
    "    \"\"\"Create an optimized GRU model for stock prediction.\"\"\"\n",
    "    model = Sequential([\n",
    "        # First GRU layer\n",
    "        GRU(units=units[0], return_sequences=True, input_shape=input_shape, \n",
    "            dropout=dropout_rate, recurrent_dropout=dropout_rate, name='gru_1'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Second GRU layer\n",
    "        GRU(units=units[1], return_sequences=True,\n",
    "            dropout=dropout_rate, recurrent_dropout=dropout_rate, name='gru_2'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Third GRU layer\n",
    "        GRU(units=units[2], return_sequences=False,\n",
    "            dropout=dropout_rate, recurrent_dropout=dropout_rate, name='gru_3'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Dense layers with regularization\n",
    "        Dense(units=32, activation='relu', name='dense_1'),\n",
    "        Dropout(dropout_rate),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Dense(units=16, activation='relu', name='dense_2'),\n",
    "        Dropout(dropout_rate/2),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(units=1, activation='linear', name='output')\n",
    "    ])\n",
    "    \n",
    "    # Compile with optimized settings\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "        loss='huber',  # More robust to outliers\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_optimized_gru_model(input_shape)\n",
    "\n",
    "print(\"Optimized GRU model created!\")\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "print(\"\\nModel architecture:\")\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
