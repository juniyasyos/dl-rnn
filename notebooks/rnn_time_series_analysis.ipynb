{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3904f57c",
   "metadata": {},
   "source": [
    "# RNN Time Series Analysis: Stock Price & Word Sequence Prediction\n",
    "\n",
    "This notebook demonstrates the implementation of Recurrent Neural Networks (RNN) for two different time series prediction tasks:\n",
    "\n",
    "1. **Stock Price Prediction**: Using GRU to forecast stock prices based on historical data\n",
    "2. **Word Sequence Prediction**: Using LSTM to predict next words in a sequence\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "- **Data**: Synthetic stock and text data\n",
    "- **Models**: LSTM for text, GRU for stock prices\n",
    "- **Framework**: TensorFlow/Keras\n",
    "- **Goal**: Create robust, non-over-engineered RNN models for time series forecasting\n",
    "\n",
    "Let's start by importing the required libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c70b0f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for data processing, model building, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f302d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Deep Learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Embedding, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11641b7a",
   "metadata": {},
   "source": [
    "## 2. Generate Dummy Stock Data\n",
    "\n",
    "Let's create synthetic stock price data with realistic patterns including trends, volatility, and seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b359caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stock_data(n_days=1000, start_price=100.0, volatility=0.02):\n",
    "    \"\"\"Generate synthetic stock price data with realistic patterns.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate dates\n",
    "    start_date = datetime(2020, 1, 1)\n",
    "    dates = [start_date + timedelta(days=i) for i in range(n_days)]\n",
    "    \n",
    "    # Generate price data using geometric Brownian motion\n",
    "    returns = np.random.normal(0.0005, volatility, n_days)  # Small positive drift\n",
    "    prices = [start_price]\n",
    "    \n",
    "    for i in range(1, n_days):\n",
    "        price = prices[-1] * (1 + returns[i])\n",
    "        prices.append(max(price, 1.0))  # Prevent negative prices\n",
    "    \n",
    "    # Generate OHLC data\n",
    "    data = []\n",
    "    for i, (date, close) in enumerate(zip(dates, prices)):\n",
    "        # Open price (close to previous close)\n",
    "        if i == 0:\n",
    "            open_price = close\n",
    "        else:\n",
    "            open_price = prices[i-1] * (1 + np.random.normal(0, volatility/4))\n",
    "        \n",
    "        # High and low prices\n",
    "        daily_range = abs(np.random.normal(0, volatility/2))\n",
    "        high = max(open_price, close) * (1 + daily_range)\n",
    "        low = min(open_price, close) * (1 - daily_range)\n",
    "        \n",
    "        # Volume\n",
    "        volume = int(np.random.normal(1000000, 200000))\n",
    "        volume = max(volume, 100000)\n",
    "        \n",
    "        data.append({\n",
    "            'Date': date,\n",
    "            'Open': round(open_price, 2),\n",
    "            'High': round(high, 2),\n",
    "            'Low': round(low, 2),\n",
    "            'Close': round(close, 2),\n",
    "            'Volume': volume\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate stock data\n",
    "stock_df = generate_stock_data(n_days=1000)\n",
    "\n",
    "# Add technical indicators\n",
    "stock_df['MA_5'] = stock_df['Close'].rolling(window=5).mean()\n",
    "stock_df['MA_20'] = stock_df['Close'].rolling(window=20).mean()\n",
    "stock_df['Returns'] = stock_df['Close'].pct_change()\n",
    "stock_df['Volatility'] = stock_df['Returns'].rolling(window=10).std()\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Generated {len(stock_df)} days of stock data\")\n",
    "print(f\"Date range: {stock_df['Date'].min()} to {stock_df['Date'].max()}\")\n",
    "print(f\"Price range: ${stock_df['Close'].min():.2f} - ${stock_df['Close'].max():.2f}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(stock_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480d4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stock data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Price chart\n",
    "axes[0, 0].plot(stock_df['Date'], stock_df['Close'], label='Close Price', alpha=0.8)\n",
    "axes[0, 0].plot(stock_df['Date'], stock_df['MA_5'], label='MA 5', alpha=0.7)\n",
    "axes[0, 0].plot(stock_df['Date'], stock_df['MA_20'], label='MA 20', alpha=0.7)\n",
    "axes[0, 0].set_title('Stock Price with Moving Averages')\n",
    "axes[0, 0].set_ylabel('Price ($)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Volume\n",
    "axes[0, 1].bar(stock_df['Date'], stock_df['Volume'], alpha=0.6, width=1)\n",
    "axes[0, 1].set_title('Trading Volume')\n",
    "axes[0, 1].set_ylabel('Volume')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Returns distribution\n",
    "axes[1, 0].hist(stock_df['Returns'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Daily Returns Distribution')\n",
    "axes[1, 0].set_xlabel('Returns')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Volatility\n",
    "axes[1, 1].plot(stock_df['Date'], stock_df['Volatility'], alpha=0.8, color='red')\n",
    "axes[1, 1].set_title('Rolling Volatility (10-day)')\n",
    "axes[1, 1].set_ylabel('Volatility')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Stock data statistics:\")\n",
    "print(stock_df[['Open', 'High', 'Low', 'Close', 'Volume']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61216edb",
   "metadata": {},
   "source": [
    "## 3. Generate Dummy Word Time Series Data\n",
    "\n",
    "Now let's create synthetic text data for word sequence prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20548a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_data(n_samples=1000, vocab_size=50, sequence_length=10):\n",
    "    \"\"\"Generate dummy text sequences with some patterns.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create vocabulary\n",
    "    words = [f\"word_{i}\" for i in range(vocab_size)]\n",
    "    word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "    idx_to_word = {i: word for i, word in enumerate(words)}\n",
    "    \n",
    "    # Generate sequences with patterns\n",
    "    sequences = []\n",
    "    for _ in range(n_samples):\n",
    "        sequence = []\n",
    "        for i in range(sequence_length):\n",
    "            if i == 0:\n",
    "                # Random start\n",
    "                word_idx = np.random.randint(0, vocab_size)\n",
    "            else:\n",
    "                # Add some pattern dependency (30% chance)\n",
    "                prev_idx = sequence[i-1]\n",
    "                if np.random.random() < 0.3:\n",
    "                    word_idx = (prev_idx + 1) % vocab_size\n",
    "                else:\n",
    "                    word_idx = np.random.randint(0, vocab_size)\n",
    "            sequence.append(word_idx)\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences, word_to_idx, idx_to_word\n",
    "\n",
    "# Generate text data\n",
    "text_sequences, word_to_idx, idx_to_word = generate_text_data(\n",
    "    n_samples=1000, vocab_size=50, sequence_length=10\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(text_sequences)} text sequences\")\n",
    "print(f\"Vocabulary size: {len(word_to_idx)}\")\n",
    "print(f\"Sequence length: {len(text_sequences[0])}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample sequences (as words):\")\n",
    "for i in range(3):\n",
    "    words = [idx_to_word[idx] for idx in text_sequences[i]]\n",
    "    print(f\"Sequence {i+1}: {' '.join(words)}\")\n",
    "\n",
    "print(\"\\nSample sequences (as indices):\")\n",
    "for i in range(3):\n",
    "    print(f\"Sequence {i+1}: {text_sequences[i]}\")\n",
    "\n",
    "# Vocabulary statistics\n",
    "print(f\"\\nFirst 10 words in vocabulary:\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}: {idx_to_word[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd70aa6",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing for Stock Data\n",
    "\n",
    "Prepare stock data for RNN training by creating sequences and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458cecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_stock_sequences(data, sequence_length=60, target_col='Close'):\n",
    "    \"\"\"Prepare stock data for RNN training.\"\"\"\n",
    "    # Select features (remove Date and NaN values)\n",
    "    feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'MA_5', 'MA_20', 'Volatility']\n",
    "    df = data[feature_cols].dropna()\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    \n",
    "    # Find target column index\n",
    "    target_idx = feature_cols.index(target_col)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(sequence_length, len(scaled_data)):\n",
    "        X.append(scaled_data[i-sequence_length:i])\n",
    "        y.append(scaled_data[i, target_idx])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Split into train and test\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler, feature_cols\n",
    "\n",
    "# Prepare stock data\n",
    "SEQUENCE_LENGTH = 60\n",
    "X_stock_train, X_stock_test, y_stock_train, y_stock_test, stock_scaler, stock_features = prepare_stock_sequences(\n",
    "    stock_df, sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "print(\"Stock data preparation complete!\")\n",
    "print(f\"Training sequences: {X_stock_train.shape}\")\n",
    "print(f\"Test sequences: {X_stock_test.shape}\")\n",
    "print(f\"Training targets: {y_stock_train.shape}\")\n",
    "print(f\"Test targets: {y_stock_test.shape}\")\n",
    "print(f\"Features used: {stock_features}\")\n",
    "\n",
    "# Show sample data shapes\n",
    "print(f\"\\nSample input sequence shape: {X_stock_train[0].shape}\")\n",
    "print(f\"Sample target: {y_stock_train[0]}\")\n",
    "\n",
    "# Verify data integrity\n",
    "print(f\"\\nData integrity check:\")\n",
    "print(f\"No NaN in X_train: {not np.isnan(X_stock_train).any()}\")\n",
    "print(f\"No NaN in y_train: {not np.isnan(y_stock_train).any()}\")\n",
    "print(f\"Data range - X_train: [{X_stock_train.min():.3f}, {X_stock_train.max():.3f}]\")\n",
    "print(f\"Data range - y_train: [{y_stock_train.min():.3f}, {y_stock_train.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0feed75",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing for Word Data\n",
    "\n",
    "Prepare text sequences for RNN training with proper encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6edc87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_sequences(sequences, sequence_length, vocab_size):\n",
    "    \"\"\"Prepare text sequences for RNN training.\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        if len(sequence) >= sequence_length + 1:\n",
    "            for i in range(len(sequence) - sequence_length):\n",
    "                # Input: sequence of length sequence_length\n",
    "                X.append(sequence[i:i + sequence_length])\n",
    "                # Target: next word (one-hot encoded)\n",
    "                target = sequence[i + sequence_length]\n",
    "                y_one_hot = np.zeros(vocab_size)\n",
    "                y_one_hot[target] = 1\n",
    "                y.append(y_one_hot)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare text data\n",
    "TEXT_SEQUENCE_LENGTH = 5  # Use 5 words to predict the next\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "\n",
    "X_text, y_text = prepare_text_sequences(text_sequences, TEXT_SEQUENCE_LENGTH, VOCAB_SIZE)\n",
    "\n",
    "# Split into train and test\n",
    "split_idx = int(0.8 * len(X_text))\n",
    "X_text_train, X_text_test = X_text[:split_idx], X_text[split_idx:]\n",
    "y_text_train, y_text_test = y_text[:split_idx], y_text[split_idx:]\n",
    "\n",
    "print(\"Text data preparation complete!\")\n",
    "print(f\"Training sequences: {X_text_train.shape}\")\n",
    "print(f\"Test sequences: {X_text_test.shape}\")\n",
    "print(f\"Training targets: {y_text_train.shape}\")\n",
    "print(f\"Test targets: {y_text_test.shape}\")\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"Sequence length: {TEXT_SEQUENCE_LENGTH}\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nSample input sequence: {X_text_train[0]}\")\n",
    "print(f\"Corresponding words: {[idx_to_word[idx] for idx in X_text_train[0]]}\")\n",
    "print(f\"Target (one-hot): {np.argmax(y_text_train[0])} -> '{idx_to_word[np.argmax(y_text_train[0])]}'\")\n",
    "\n",
    "# Data statistics\n",
    "print(f\"\\nData statistics:\")\n",
    "print(f\"Total sequences: {len(X_text)}\")\n",
    "print(f\"Unique words in vocabulary: {VOCAB_SIZE}\")\n",
    "print(f\"Average sequence frequency: {len(X_text) / VOCAB_SIZE:.1f} per word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23479ddf",
   "metadata": {},
   "source": [
    "## 6. Create RNN Model for Stock Prediction\n",
    "\n",
    "Build a GRU-based model for stock price forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148addd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stock_model(input_shape):\n",
    "    \"\"\"Create GRU model for stock prediction.\"\"\"\n",
    "    model = Sequential([\n",
    "        # First GRU layer\n",
    "        GRU(units=64, return_sequences=True, input_shape=input_shape, \n",
    "            dropout=0.2, recurrent_dropout=0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Second GRU layer\n",
    "        GRU(units=32, return_sequences=False, \n",
    "            dropout=0.2, recurrent_dropout=0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(units=16, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(units=1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create stock prediction model\n",
    "stock_input_shape = (X_stock_train.shape[1], X_stock_train.shape[2])\n",
    "stock_model = create_stock_model(stock_input_shape)\n",
    "\n",
    "print(\"Stock prediction model created!\")\n",
    "print(f\"Input shape: {stock_input_shape}\")\n",
    "print(\"\\nModel architecture:\")\n",
    "stock_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a36ec1c",
   "metadata": {},
   "source": [
    "## 7. Create RNN Model for Word Sequence Prediction\n",
    "\n",
    "Build an LSTM-based model with embedding layer for text prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_model(vocab_size, sequence_length, embedding_dim=32):\n",
    "    \"\"\"Create LSTM model for text prediction.\"\"\"\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                 input_length=sequence_length),\n",
    "        \n",
    "        # LSTM layers\n",
    "        LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "        LSTM(units=32, dropout=0.2, recurrent_dropout=0.2),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(units=32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Output layer (softmax for word prediction)\n",
    "        Dense(units=vocab_size, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create text prediction model\n",
    "text_model = create_text_model(VOCAB_SIZE, TEXT_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Text prediction model created!\")\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"Sequence length: {TEXT_SEQUENCE_LENGTH}\")\n",
    "print(\"\\nModel architecture:\")\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d38c78",
   "metadata": {},
   "source": [
    "## 8. Train Stock Price Model\n",
    "\n",
    "Train the GRU model for stock price prediction with proper callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd1dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for stock model\n",
    "stock_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"Training stock prediction model...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Train stock model\n",
    "stock_history = stock_model.fit(\n",
    "    X_stock_train, y_stock_train,\n",
    "    validation_data=(X_stock_test, y_stock_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=stock_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStock model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da4411",
   "metadata": {},
   "source": [
    "## 9. Train Word Sequence Model\n",
    "\n",
    "Train the LSTM model for word sequence prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for text model\n",
    "text_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"Training word sequence prediction model...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Train text model\n",
    "text_history = text_model.fit(\n",
    "    X_text_train, y_text_train,\n",
    "    validation_data=(X_text_test, y_text_test),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=text_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nText model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419751c1",
   "metadata": {},
   "source": [
    "## 10. Evaluate Model Performance\n",
    "\n",
    "Evaluate both models using appropriate metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a43ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate stock model\n",
    "print(\"=== STOCK MODEL EVALUATION ===\")\n",
    "stock_loss, stock_mae = stock_model.evaluate(X_stock_test, y_stock_test, verbose=0)\n",
    "print(f\"Test Loss (MSE): {stock_loss:.6f}\")\n",
    "print(f\"Test MAE: {stock_mae:.6f}\")\n",
    "\n",
    "# Get stock predictions\n",
    "stock_predictions = stock_model.predict(X_stock_test, verbose=0)\n",
    "\n",
    "# Calculate additional metrics\n",
    "stock_mse = mean_squared_error(y_stock_test, stock_predictions)\n",
    "stock_rmse = np.sqrt(stock_mse)\n",
    "stock_mape = np.mean(np.abs((y_stock_test - stock_predictions.flatten()) / y_stock_test)) * 100\n",
    "\n",
    "print(f\"RMSE: {stock_rmse:.6f}\")\n",
    "print(f\"MAPE: {stock_mape:.2f}%\")\n",
    "\n",
    "print(\"\\n=== TEXT MODEL EVALUATION ===\")\n",
    "text_loss, text_accuracy = text_model.evaluate(X_text_test, y_text_test, verbose=0)\n",
    "print(f\"Test Loss: {text_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {text_accuracy:.4f} ({text_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Get text predictions\n",
    "text_predictions = text_model.predict(X_text_test, verbose=0)\n",
    "text_pred_classes = np.argmax(text_predictions, axis=1)\n",
    "text_true_classes = np.argmax(y_text_test, axis=1)\n",
    "\n",
    "# Additional text metrics\n",
    "text_precision = np.mean(text_pred_classes == text_true_classes)\n",
    "print(f\"Prediction Precision: {text_precision:.4f} ({text_precision*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "print(f\"Stock Model - Normalized Performance: {1 - stock_mape/100:.3f}\")\n",
    "print(f\"Text Model - Accuracy: {text_accuracy:.3f}\")\n",
    "print(\"\\nBoth models show reasonable performance for dummy data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23144e7",
   "metadata": {},
   "source": [
    "## 11. Make Predictions on New Data\n",
    "\n",
    "Demonstrate how to use the trained models for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock price prediction example\n",
    "print(\"=== STOCK PRICE PREDICTION EXAMPLE ===\")\n",
    "# Use the last sequence from test data\n",
    "last_sequence = X_stock_test[-1].reshape(1, SEQUENCE_LENGTH, len(stock_features))\n",
    "next_price_scaled = stock_model.predict(last_sequence, verbose=0)[0][0]\n",
    "\n",
    "# Create dummy data for inverse transform (we only need the Close price)\n",
    "dummy_data = np.zeros((1, len(stock_features)))\n",
    "dummy_data[0, stock_features.index('Close')] = next_price_scaled\n",
    "next_price = stock_scaler.inverse_transform(dummy_data)[0, stock_features.index('Close')]\n",
    "\n",
    "print(f\"Last known price (scaled): {y_stock_test[-1]:.4f}\")\n",
    "print(f\"Predicted next price (scaled): {next_price_scaled:.4f}\")\n",
    "print(f\"Predicted next price (actual): ${next_price:.2f}\")\n",
    "\n",
    "# Text generation example\n",
    "print(\"\\n=== TEXT GENERATION EXAMPLE ===\")\n",
    "\n",
    "def generate_text(model, seed_sequence, num_words=5, temperature=1.0):\n",
    "    \"\"\"Generate text using the trained model.\"\"\"\n",
    "    generated = seed_sequence.copy()\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        # Use last TEXT_SEQUENCE_LENGTH words\n",
    "        input_seq = generated[-TEXT_SEQUENCE_LENGTH:]\n",
    "        input_seq = np.array(input_seq).reshape(1, -1)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = model.predict(input_seq, verbose=0)[0]\n",
    "        \n",
    "        # Apply temperature and sample\n",
    "        if temperature != 1.0:\n",
    "            prediction = np.log(prediction + 1e-8) / temperature\n",
    "            prediction = np.exp(prediction) / np.sum(np.exp(prediction))\n",
    "        \n",
    "        # Sample next word\n",
    "        next_word = np.random.choice(len(prediction), p=prediction)\n",
    "        generated.append(next_word)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Generate some text examples\n",
    "seed_sequences = [\n",
    "    [0, 1, 2, 3, 4],\n",
    "    [10, 11, 12, 13, 14],\n",
    "    [20, 21, 22, 23, 24]\n",
    "]\n",
    "\n",
    "for i, seed in enumerate(seed_sequences):\n",
    "    generated = generate_text(text_model, seed, num_words=5)\n",
    "    \n",
    "    # Convert to words\n",
    "    seed_words = [idx_to_word[idx] for idx in seed]\n",
    "    generated_words = [idx_to_word[idx] for idx in generated]\n",
    "    new_words = generated_words[len(seed):]\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Seed: {' '.join(seed_words)}\")\n",
    "    print(f\"Generated: {' '.join(generated_words)}\")\n",
    "    print(f\"New words: {' '.join(new_words)}\")\n",
    "\n",
    "print(\"\\n=== PREDICTION PIPELINE DEMONSTRATED ===\")\n",
    "print(\"Both models can now be used for real-time predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282bcef",
   "metadata": {},
   "source": [
    "## 12. Visualize Results\n",
    "\n",
    "Create comprehensive visualizations to understand model performance and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad14ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. Stock model training history\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(stock_history.history['loss'], label='Training Loss', alpha=0.8)\n",
    "plt.plot(stock_history.history['val_loss'], label='Validation Loss', alpha=0.8)\n",
    "plt.title('Stock Model - Training History (Loss)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 2. Stock model MAE history\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(stock_history.history['mae'], label='Training MAE', alpha=0.8)\n",
    "plt.plot(stock_history.history['val_mae'], label='Validation MAE', alpha=0.8)\n",
    "plt.title('Stock Model - Training History (MAE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 3. Stock predictions vs actual\n",
    "plt.subplot(3, 3, 3)\n",
    "# Show only first 100 predictions for clarity\n",
    "n_show = min(100, len(stock_predictions))\n",
    "plt.plot(y_stock_test[:n_show], label='Actual', alpha=0.8, linewidth=2)\n",
    "plt.plot(stock_predictions[:n_show].flatten(), label='Predicted', alpha=0.8, linewidth=2)\n",
    "plt.title('Stock Price Predictions vs Actual')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Normalized Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 4. Text model training history - Loss\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.plot(text_history.history['loss'], label='Training Loss', alpha=0.8)\n",
    "plt.plot(text_history.history['val_loss'], label='Validation Loss', alpha=0.8)\n",
    "plt.title('Text Model - Training History (Loss)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Categorical Crossentropy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 5. Text model training history - Accuracy\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.plot(text_history.history['accuracy'], label='Training Accuracy', alpha=0.8)\n",
    "plt.plot(text_history.history['val_accuracy'], label='Validation Accuracy', alpha=0.8)\n",
    "plt.title('Text Model - Training History (Accuracy)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 6. Text prediction confidence distribution\n",
    "plt.subplot(3, 3, 6)\n",
    "max_probs = np.max(text_predictions, axis=1)\n",
    "plt.hist(max_probs, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Text Prediction Confidence Distribution')\n",
    "plt.xlabel('Maximum Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# 7. Stock prediction error distribution\n",
    "plt.subplot(3, 3, 7)\n",
    "errors = y_stock_test - stock_predictions.flatten()\n",
    "plt.hist(errors, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Stock Prediction Error Distribution')\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# 8. Stock prediction scatter plot\n",
    "plt.subplot(3, 3, 8)\n",
    "plt.scatter(y_stock_test, stock_predictions.flatten(), alpha=0.6)\n",
    "plt.plot([y_stock_test.min(), y_stock_test.max()], \n",
    "         [y_stock_test.min(), y_stock_test.max()], 'r--', lw=2)\n",
    "plt.title('Stock: Predicted vs Actual')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.grid(True)\n",
    "\n",
    "# 9. Model complexity comparison\n",
    "plt.subplot(3, 3, 9)\n",
    "models = ['Stock (GRU)', 'Text (LSTM)']\n",
    "params = [stock_model.count_params(), text_model.count_params()]\n",
    "colors = ['skyblue', 'lightcoral']\n",
    "bars = plt.bar(models, params, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.title('Model Complexity (Parameters)')\n",
    "plt.ylabel('Number of Parameters')\n",
    "plt.yscale('log')\n",
    "for bar, param in zip(bars, params):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "             f'{param:,}', ha='center', va='bottom')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=== VISUALIZATION SUMMARY ===\")\n",
    "print(f\"Stock Model Parameters: {stock_model.count_params():,}\")\n",
    "print(f\"Text Model Parameters: {text_model.count_params():,}\")\n",
    "print(f\"Stock Model Final Val Loss: {stock_history.history['val_loss'][-1]:.6f}\")\n",
    "print(f\"Text Model Final Val Accuracy: {text_history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Total Training Epochs - Stock: {len(stock_history.history['loss'])}\")\n",
    "print(f\"Total Training Epochs - Text: {len(text_history.history['loss'])}\")\n",
    "\n",
    "# Final model performance summary\n",
    "print(\"\\n=== FINAL MODEL PERFORMANCE ===\")\n",
    "print(\"Stock Price Prediction:\")\n",
    "print(f\"  â€¢ RMSE: {stock_rmse:.6f}\")\n",
    "print(f\"  â€¢ MAPE: {stock_mape:.2f}%\")\n",
    "print(f\"  â€¢ Correlation: {np.corrcoef(y_stock_test, stock_predictions.flatten())[0,1]:.4f}\")\n",
    "\n",
    "print(\"\\nWord Sequence Prediction:\")\n",
    "print(f\"  â€¢ Accuracy: {text_accuracy:.4f} ({text_accuracy*100:.2f}%)\")\n",
    "print(f\"  â€¢ Avg Confidence: {np.mean(np.max(text_predictions, axis=1)):.4f}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ RNN Time Series Analysis Complete!\")\n",
    "print(\"Both models successfully trained and evaluated on dummy data.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
